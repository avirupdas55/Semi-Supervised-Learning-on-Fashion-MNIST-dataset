{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac47551d-2b53-44e4-8c5c-289f1f80c3cd",
   "metadata": {},
   "source": [
    "# Using kmeans clustering to identify a small subset of labelled images to seed the classification process for Fashion MNIST dataset\n",
    "## -Avirup Das [MDS202013]\n",
    "## -Ayush Thada [MDS202014]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e03c1a6-0299-43d7-b6cc-f53605346b5a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Clustering is usually used for problems related to unsupervised learning but we will use it as a pre-processing tool for semi-supervised learning. If we only have a few labels, we could perform clustering and propagate the labels to all the instances (or to the closest instances decided by percentile) in the same cluster. This technique can greatly increase the number of labels available for a subsequent supervised learning algorithm, and thus improve its performance.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "We will use logistic regression and a 7 layer deep neural network to classify the Fashion MNIST dataset. For each of these models we would first train the model with the whole data set (60000 instances) and test it for 10000 instances. The accuracy will be our baseline and we would try to improve upon that.<br>\n",
    "\n",
    "|        Model        | Baseline Accuracy |\n",
    "|:-------------------:|:-----------------:|\n",
    "| Logistic Regression |       84.1%       |\n",
    "|    Neural Network   |       89.68%      |\n",
    "\n",
    "\n",
    "The Fashion MNIST dataset contains images of dimension $28\\times28$, so as a pre-processing step for Logistic regression, we flatten the image matrix into a vector. We have also pickled our models so they can be re-used without retraining as we observed that the training time was very high since we were also experimenting with different values of the number of clusters. These files will be provided along with the code.\n",
    "\n",
    "First we have created a pipeline that will cluster the training set into 100, 200 and 300 clusters and replace the images with their distances to these clusters, then apply a Logistic Regression model. \n",
    "\n",
    "| Cluster | Accuracy |\n",
    "|:-------:|:--------:|\n",
    "|   100   |  82.62%  |\n",
    "|   200   |  83.89%  |\n",
    "|   300   |  84.56%  |\n",
    "\n",
    "Then we took random n-labelled instances (for n=500, 1000, 2000) and check how our models perform in terms of accuracy.\n",
    "\n",
    "| Cluster | Logistic Regression | Neural Network |\n",
    "|:-------:|:-------------------:|:--------------:|\n",
    "|   500   |        78.52%       |     73.51%     |\n",
    "|   1000  |        79.24%       |       76%      |\n",
    "|   2000  |        80.89%       |     78.53%     |\n",
    "\n",
    "We can see that the neural network does not perform very well compared to Logistic Regression in this case since we are using a very small amount of data as training set. On the other hand, the neural network takes much lesser time to get trained. We also see that the models perform better for large cluster sizes suggesting that for further experimentation we should work with large cluster sizes.<br>\n",
    "\n",
    "Next, we cluster the instances into 2000 clusters and use the centroids to train our model.\n",
    "\n",
    "| Cluster | Logistic Regression | Neural Network |\n",
    "|:-------:|:-------------------:|:--------------:|\n",
    "|   500   |          -          |      75.5%     |\n",
    "|   1000  |          -          |     76.36%     |\n",
    "|   2000  |        81.46%       |     80.24%     |\n",
    "\n",
    "Again we see that the accuracy increases when the number of clusters is increased. Now we propagate the labels of these representative points (centroids) to all the instances under the same cluster and run our models.\n",
    "\n",
    "| Cluster | Logistic Regression | Neural Network |\n",
    "|:-------:|:-------------------:|:--------------:|\n",
    "|   500   |          -          |     76.49%     |\n",
    "|   1000  |          -          |     77.80%     |\n",
    "|   2000  |        81.21%       |     79.90%     |\n",
    "\n",
    "\n",
    "We do not see a significant difference between the results that is because when we have propagated the labels of the centroids to all instances, we have also included outliers or instances which are ambiguous in terms of which cluster they fall in. So let us propagate the labels to the instances which are close (25 percentile) to the cluster centroids an train our models again.\n",
    "\n",
    "| Cluster | Logistic Regression | Neural Network |\n",
    "|:-------:|:-------------------:|:--------------:|\n",
    "|   500   |          -          |     75.98%     |\n",
    "|   1000  |          -          |     77.34%     |\n",
    "|   2000  |        80.44%       |     79.42%     |\n",
    "\n",
    "\n",
    "Finally for 2000 clusters we try to find the optimum distance from the centroid (in terms of percentile) so as to achieve maximum accuracy.\n",
    "\n",
    "| Percentile Distance | NN Accuracy |\n",
    "|:-------------------:|:-----------:|\n",
    "|          20         |     ~10%    |\n",
    "|          25         |    79.42%   |\n",
    "|          30         |    80.12%   |\n",
    "|          50         |    80.12%   |\n",
    "|          75         |    79.60%   |\n",
    "\n",
    "\n",
    "We can see that the optimum distance is around 30-50th percentile after which the accuracy drops (from both ends of the range). It is to be noted that although we could not improve upon our baseline accuracy using semi-supervised learning techniques, but in a situation where we get completely unlabelled data, these techniques come handy for boosting the accuracy of our models after we have labelled a small but somewhat significant portion of the data manually.<br><br>\n",
    "**Note that the accuracy scores for neural network may vary due to random initialisations or GPU configuration** <br><br>\n",
    "Link to output folder: https://mega.nz/folder/QHhyADyK#1rk56JFrTMZ-RXJpjzXKAg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b04cbd36-4646-48c2-8ade-4e5b8c0332f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82abf403-133f-4311-8441-c6c940ab4614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers, losses, metrics, callbacks, datasets\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from joblib import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "seed=42\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b015c9a9-fa52-471c-af5a-b9cb801683af",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "604276cb-8f1a-4b53-bb1e-3a647c90004d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-train shape: (60000, 28, 28)\n",
      "y-train shape: (60000,)\n",
      "X-test shape: (10000, 28, 28)\n",
      "y-test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train,y_train),(X_test,y_test)=datasets.fashion_mnist.load_data()\n",
    "class_names=['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt',\n",
    "            'Sneaker','Bag','Ankle Boot']\n",
    "# Normalizing\n",
    "X_train, X_test= X_train/255.0, X_test/255.0\n",
    "\n",
    "# Checking shape of the data\n",
    "print(f\"X-train shape: {X_train.shape}\")\n",
    "print(f\"y-train shape: {y_train.shape}\")\n",
    "print(f\"X-test shape: {X_test.shape}\")\n",
    "print(f\"y-test shape: {y_test.shape}\")\n",
    "\n",
    "# Flattening data for Logistic Regression\n",
    "X_train_flatten= X_train.reshape(X_train.shape[0],-1)\n",
    "y_train_flatten= y_train.reshape(y_train.shape[0],-1)\n",
    "X_test_flatten= X_test.reshape(X_test.shape[0],-1)\n",
    "y_test_flatten= y_test.reshape(y_test.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb103a1-9392-45a1-bd40-7d1269305c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a  Shirt\n"
     ]
    }
   ],
   "source": [
    "index=3000\n",
    "plt.imshow(X_train[index],cmap='binary')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print('It is a ',class_names[y_train[index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647a12d1-806f-4cd3-a67b-ae25934d402e",
   "metadata": {},
   "source": [
    "# Experimenting with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da93171a-9a2b-4b3b-884e-0184cfe761d0",
   "metadata": {},
   "source": [
    "## On original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6b1ef47-5d0f-41c6-b835-7f74c36d4632",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3220.67 MiB, increment: 0.00 MiB\n",
      "CPU times: user 31min 35s, sys: 4.14 s, total: 31min 40s\n",
      "Wall time: 31min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "log_reg1 = LogisticRegression(multi_class=\"ovr\", solver=\"saga\", max_iter=5000, random_state=seed)\n",
    "log_reg1.fit(X_train_flatten, y_train_flatten.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b45153b0-1929-4c65-a5bc-c21bff85862a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.841"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(log_reg1,'log_reg_orig.joblib')\n",
    "log_reg1.score(X_test_flatten, y_test_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "896a4048-6ade-4052-8d05-a895c0a224f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster= 100, Accuracy= 0.8262\n",
      "Cluster= 200, Accuracy= 0.8389\n",
      "Cluster= 300, Accuracy= 0.8456\n",
      "peak memory: 6364.94 MiB, increment: 3626.22 MiB\n",
      "CPU times: user 3h 16min 20s, sys: 2min 29s, total: 3h 18min 49s\n",
      "Wall time: 2h 47min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "k=[100,200,300]\n",
    "pipeline=np.empty(len(k),dtype=object)\n",
    "for i in range(len(k)):\n",
    "    pipeline[i] = Pipeline([(\"kmeans\", KMeans(init='k-means++',n_clusters=k[i], random_state=seed)),\n",
    "                         (\"log_reg\", LogisticRegression(multi_class=\"ovr\", solver=\"saga\",\n",
    "                                                        max_iter=5000, random_state=seed))])\n",
    "    pipeline[i].fit(X_train_flatten, y_train_flatten.ravel())\n",
    "    dump(pipeline[i],'log_reg_kmeans_{}.joblib'.format(i))\n",
    "    print('Cluster= {}, Accuracy= {}'.format(k[i],\n",
    "            pipeline[i].score(X_test_flatten,y_test_flatten)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e348d1-d93c-4112-8ac1-045726532eb7",
   "metadata": {},
   "source": [
    "## Random Labelled Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f020c5ab-a48f-4b1b-aadf-6b77eb893b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelled Instance= 500, Accuracy= 0.7852\n",
      "Labelled Instance= 1000, Accuracy= 0.7924\n",
      "Labelled Instance= 2000, Accuracy= 0.8089\n",
      "peak memory: 1732.44 MiB, increment: 7.16 MiB\n",
      "CPU times: user 17.2 s, sys: 518 ms, total: 17.7 s\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "k=[500,1000,2000]\n",
    "log_reg2=np.empty(len(k),dtype=object)\n",
    "for i in range(len(k)):\n",
    "    log_reg2[i] = LogisticRegression(multi_class=\"ovr\", solver=\"saga\", random_state=seed)\n",
    "    log_reg2[i].fit(X_train_flatten[:k[i]], y_train_flatten[:k[i]].ravel())\n",
    "    dump(log_reg2[i],'log_reg_few_label_{}.joblib'.format(i))\n",
    "    print('Labelled Instance= {}, Accuracy= {}'.format(k[i],\n",
    "            log_reg2[i].score(X_test_flatten,y_test_flatten.ravel())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53eacca7-4eb1-4d29-98ca-fbcb02e125e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 10396.50 MiB, increment: 8859.62 MiB\n",
      "CPU times: user 1h 32min 8s, sys: 7min 11s, total: 1h 39min 19s\n",
      "Wall time: 16min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "kmeans1 = KMeans(init='k-means++',n_clusters=2000, random_state=seed)\n",
    "X_items_dist = kmeans1.fit_transform(X_train_flatten)\n",
    "dump(kmeans1,'kmeans_2000.joblib')\n",
    "representative_items_idx = np.argmin(X_items_dist, axis=0)\n",
    "X_representative_items = X_train_flatten[representative_items_idx]\n",
    "y_representative_items = np.squeeze(y_train_flatten[representative_items_idx]).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62a5da67-6362-4b2a-941f-cbf9d896d3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ankle Boot' 'T-shirt/top' 'T-shirt/top' ... 'Bag' 'Trouser' 'Ankle Boot']\n"
     ]
    }
   ],
   "source": [
    "y_representative_items = np.squeeze(y_train_flatten[representative_items_idx]).astype('int32')\n",
    "print(np.array(class_names)[y_representative_items])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cbc167-a7e1-41ad-bc86-f6d9cc8cf2df",
   "metadata": {},
   "source": [
    "## Using These Centroids to fit logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a607fcac-ce2e-4e4e-95a6-5fae5e4da8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=  0.8146\n",
      "peak memory: 8181.33 MiB, increment: 0.03 MiB\n",
      "CPU times: user 59.9 s, sys: 160 ms, total: 1min\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "log_reg3 = LogisticRegression(multi_class=\"ovr\", solver=\"saga\", max_iter=5000, random_state=seed)\n",
    "log_reg3.fit(X_representative_items, y_representative_items)\n",
    "dump(log_reg3,'log_reg_centroids.joblib')\n",
    "print('Accuracy= ',log_reg3.score(X_test_flatten, y_test_flatten))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f0a968-fe0e-4f97-9b00-ce875a2de473",
   "metadata": {},
   "source": [
    "## Clustering and Propgating Labels to each Data Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed4b1f35-ef9c-4873-962b-1761eaae0a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_propagated = np.empty(len(X_train_flatten), dtype=np.int32)\n",
    "for i in range(2000):\n",
    "    y_train_propagated[kmeans1.labels_==i] = y_representative_items[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f73c808-0414-4d9b-ade3-4c90ee3b2f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8121\n",
      "peak memory: 8182.73 MiB, increment: 0.02 MiB\n",
      "CPU times: user 32min 4s, sys: 3.96 s, total: 32min 8s\n",
      "Wall time: 32min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "log_reg4 = LogisticRegression(multi_class=\"ovr\", solver=\"saga\", max_iter=5000, random_state=seed)\n",
    "log_reg4.fit(X_train_flatten, y_train_propagated)\n",
    "dump(log_reg4,'log_reg_propagated.joblib')\n",
    "print(log_reg4.score(X_test_flatten,y_test_flatten))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b016a5-04cf-4f5a-81dd-abd3a53f51cf",
   "metadata": {},
   "source": [
    "## Clustering and Propgating Labels to Data Points Closer to Cluster Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68ed1b2f-83b8-4305-a59f-9ca4acf2d134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3609.66 MiB, increment: 895.62 MiB\n",
      "CPU times: user 264 ms, sys: 262 ms, total: 526 ms\n",
      "Wall time: 646 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "percentile_closest = 25\n",
    "\n",
    "X_cluster_dist = X_items_dist[np.arange(len(X_train_flatten)), kmeans1.labels_]\n",
    "for i in range(2000):\n",
    "    in_cluster = (kmeans1.labels_ == i)\n",
    "    cluster_dist = X_cluster_dist[in_cluster]\n",
    "    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
    "    above_cutoff = (X_cluster_dist > cutoff_distance)\n",
    "    X_cluster_dist[in_cluster & above_cutoff] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d4b973b-ebbc-4f25-bba2-bd35deb6649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "partially_propagated = (X_cluster_dist != -1)\n",
    "X_train_partially_propagated = X_train_flatten[partially_propagated]\n",
    "y_train_partially_propagated = y_train_propagated[partially_propagated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5ad35ff-548a-4991-b0ec-fcccaae83166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8044\n",
      "peak memory: 4021.36 MiB, increment: 0.03 MiB\n",
      "CPU times: user 7min 15s, sys: 737 ms, total: 7min 16s\n",
      "Wall time: 7min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "log_reg5 = LogisticRegression(multi_class=\"ovr\", solver=\"saga\", max_iter=5000, random_state=seed)\n",
    "log_reg5.fit(X_train_partially_propagated, y_train_partially_propagated)\n",
    "dump(log_reg5,'log_reg_partially_propagated.joblib')\n",
    "print(log_reg5.score(X_test_flatten,y_test_flatten))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ab3d38-9f7a-4422-aa55-cd9a14b2c079",
   "metadata": {},
   "source": [
    "# Experimenting with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd291b36-494e-41fc-98f8-9016a7b5122c",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e8361c9-bfb9-462a-a875-683c37f12dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape=(28,28, 1), n_class=10):\n",
    "    model = models.Sequential([\n",
    "                               layers.InputLayer(input_shape=input_shape),\n",
    "                               layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "                               layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "                               layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "                               layers.Conv1D(1, 1, activation=\"relu\"),\n",
    "                               layers.Flatten(),\n",
    "                               layers.Dense(64, activation='relu'),\n",
    "                               layers.Dense(10, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    model.compile(loss=losses.SparseCategoricalCrossentropy(name=\"loss\"), \n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, x_data, y_data, patience=8, batch_size=32):\n",
    "    estop = callbacks.EarlyStopping(patience=patience)\n",
    "    history = model.fit(x_data[..., np.newaxis], \n",
    "                        y_data, \n",
    "                        batch_size = batch_size, \n",
    "                        epochs = 100,\n",
    "                        verbose = 1,\n",
    "                        validation_split = 0.1,\n",
    "                        callbacks = [estop])\n",
    "    return history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d58662-dbd6-4195-b8ab-ce277689723c",
   "metadata": {},
   "source": [
    "## On Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f26c366f-e416-42fe-b2ee-2f7ac8204791",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x168c4bdc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x168c4bdc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1686/1688 [============================>.] - ETA: 0s - loss: 2.3028 - accuracy: 0.0969WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x1695eb280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x1695eb280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1688/1688 [==============================] - 43s 25ms/step - loss: 2.3028 - accuracy: 0.0969 - val_loss: 2.3027 - val_accuracy: 0.0985\n",
      "Epoch 2/100\n",
      "1688/1688 [==============================] - 41s 24ms/step - loss: 2.3027 - accuracy: 0.0984 - val_loss: 2.3030 - val_accuracy: 0.0942\n",
      "Epoch 3/100\n",
      "1688/1688 [==============================] - 41s 24ms/step - loss: 2.3028 - accuracy: 0.1005 - val_loss: 2.3030 - val_accuracy: 0.0985\n",
      "Epoch 4/100\n",
      "1688/1688 [==============================] - 41s 24ms/step - loss: 2.3027 - accuracy: 0.1026 - val_loss: 2.3029 - val_accuracy: 0.0925\n",
      "Epoch 5/100\n",
      "1688/1688 [==============================] - 41s 24ms/step - loss: 2.3027 - accuracy: 0.0979 - val_loss: 2.3027 - val_accuracy: 0.1008\n",
      "Epoch 6/100\n",
      "1688/1688 [==============================] - 42s 25ms/step - loss: 2.3027 - accuracy: 0.0957 - val_loss: 2.3029 - val_accuracy: 0.0942\n",
      "Epoch 7/100\n",
      "1688/1688 [==============================] - 42s 25ms/step - loss: 2.3028 - accuracy: 0.0986 - val_loss: 2.3026 - val_accuracy: 0.1032\n",
      "Epoch 8/100\n",
      "1688/1688 [==============================] - 42s 25ms/step - loss: 2.3028 - accuracy: 0.1002 - val_loss: 2.3028 - val_accuracy: 0.0925\n",
      "Epoch 9/100\n",
      "1688/1688 [==============================] - 41s 24ms/step - loss: 2.3027 - accuracy: 0.0978 - val_loss: 2.3027 - val_accuracy: 0.0985\n",
      "Epoch 10/100\n",
      "1688/1688 [==============================] - 41s 24ms/step - loss: 2.3028 - accuracy: 0.0973 - val_loss: 2.3028 - val_accuracy: 0.1027\n",
      "Epoch 11/100\n",
      "1688/1688 [==============================] - 43s 25ms/step - loss: 2.3027 - accuracy: 0.0997 - val_loss: 2.3029 - val_accuracy: 0.0973\n",
      "Epoch 12/100\n",
      "1688/1688 [==============================] - 43s 25ms/step - loss: 2.3027 - accuracy: 0.0976 - val_loss: 2.3030 - val_accuracy: 0.0942\n",
      "Epoch 13/100\n",
      "1688/1688 [==============================] - 42s 25ms/step - loss: 2.3027 - accuracy: 0.1004 - val_loss: 2.3029 - val_accuracy: 0.0925\n",
      "Epoch 14/100\n",
      "1688/1688 [==============================] - 42s 25ms/step - loss: 2.3028 - accuracy: 0.0965 - val_loss: 2.3029 - val_accuracy: 0.0925\n",
      "Epoch 15/100\n",
      "1688/1688 [==============================] - 41s 25ms/step - loss: 2.3027 - accuracy: 0.1003 - val_loss: 2.3028 - val_accuracy: 0.0925\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "peak memory: 1793.67 MiB, increment: 299.08 MiB\n",
      "CPU times: user 17min 3s, sys: 35.8 s, total: 17min 39s\n",
      "Wall time: 10min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "model_1 = get_model()\n",
    "display(plot_model(model_1, rankdir=\"LR\", show_shapes=True, show_layer_names=False))\n",
    "\n",
    "# Train Model\n",
    "history_1 = train_model(model_1, X_train, y_train);\n",
    "# Testing Evaluation\n",
    "model_1.evaluate(X_test[..., np.newaxis], y_test);\n",
    "model_1.save('nn_original')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad2a783-79b3-4fb8-8697-06840200c060",
   "metadata": {},
   "source": [
    "## Random Labelled Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed0ce8d6-ecf2-48da-ac06-d90495c4ec7a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x168c698b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x168c698b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "13/15 [=========================>....] - ETA: 0s - loss: 2.3027 - accuracy: 0.0873WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x1695d2af0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x1695d2af0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "15/15 [==============================] - 1s 32ms/step - loss: 2.3027 - accuracy: 0.0876 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.3021 - accuracy: 0.0982 - val_loss: 2.3039 - val_accuracy: 0.1400\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 2.3019 - accuracy: 0.1064 - val_loss: 2.3052 - val_accuracy: 0.1400\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 2.3009 - accuracy: 0.0973 - val_loss: 2.3068 - val_accuracy: 0.0800\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.3020 - accuracy: 0.0860 - val_loss: 2.3080 - val_accuracy: 0.0800\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.3000 - accuracy: 0.0738 - val_loss: 2.3098 - val_accuracy: 0.0800\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.3010 - accuracy: 0.1180 - val_loss: 2.3105 - val_accuracy: 0.0800\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 2.3000 - accuracy: 0.0968 - val_loss: 2.3118 - val_accuracy: 0.0400\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 2.2982 - accuracy: 0.1392 - val_loss: 2.3126 - val_accuracy: 0.0400\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 2.3033 - accuracy: 0.1000\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2af4e5670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2af4e5670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: nn_labelled_500/assets\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2bdb21550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2bdb21550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "27/29 [==========================>...] - ETA: 0s - loss: 1.9390 - accuracy: 0.3460WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2bdb21940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2bdb21940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "29/29 [==============================] - 1s 29ms/step - loss: 1.9025 - accuracy: 0.3601 - val_loss: 1.2550 - val_accuracy: 0.5600\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 1.0264 - accuracy: 0.6355 - val_loss: 0.9964 - val_accuracy: 0.5600\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 1s 26ms/step - loss: 0.8426 - accuracy: 0.6930 - val_loss: 0.8778 - val_accuracy: 0.7500\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 1s 26ms/step - loss: 0.7261 - accuracy: 0.7494 - val_loss: 0.7711 - val_accuracy: 0.6900\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 1s 26ms/step - loss: 0.7342 - accuracy: 0.7159 - val_loss: 0.6388 - val_accuracy: 0.7700\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 1s 26ms/step - loss: 0.5638 - accuracy: 0.8022 - val_loss: 0.6656 - val_accuracy: 0.7800\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.6655 - accuracy: 0.7584 - val_loss: 0.6358 - val_accuracy: 0.7800\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 1s 26ms/step - loss: 0.5561 - accuracy: 0.8294 - val_loss: 0.9327 - val_accuracy: 0.7300\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.7704 - accuracy: 0.7820 - val_loss: 0.5686 - val_accuracy: 0.8100\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.4528 - accuracy: 0.8436 - val_loss: 0.6628 - val_accuracy: 0.7700\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 1s 26ms/step - loss: 0.5868 - accuracy: 0.7824 - val_loss: 0.7054 - val_accuracy: 0.7900\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.4928 - accuracy: 0.8265 - val_loss: 0.5520 - val_accuracy: 0.8300\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.5128 - accuracy: 0.8176 - val_loss: 0.5729 - val_accuracy: 0.7900\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.3541 - accuracy: 0.8687 - val_loss: 0.6482 - val_accuracy: 0.8100\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.2682 - accuracy: 0.9083 - val_loss: 0.5545 - val_accuracy: 0.8400\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 1s 24ms/step - loss: 0.3468 - accuracy: 0.8763 - val_loss: 0.5290 - val_accuracy: 0.8100\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.2729 - accuracy: 0.8890 - val_loss: 0.5686 - val_accuracy: 0.8600\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 1s 24ms/step - loss: 0.2277 - accuracy: 0.9205 - val_loss: 0.6020 - val_accuracy: 0.8300\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 1s 24ms/step - loss: 0.3367 - accuracy: 0.8914 - val_loss: 0.5597 - val_accuracy: 0.8200\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 1s 24ms/step - loss: 0.2062 - accuracy: 0.9215 - val_loss: 0.6378 - val_accuracy: 0.8400\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.1697 - accuracy: 0.9358 - val_loss: 0.6930 - val_accuracy: 0.8100\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 1s 26ms/step - loss: 0.1606 - accuracy: 0.9391 - val_loss: 0.6325 - val_accuracy: 0.8100\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.1633 - accuracy: 0.9357 - val_loss: 0.7972 - val_accuracy: 0.8300\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.1816 - accuracy: 0.9315 - val_loss: 0.6911 - val_accuracy: 0.8500\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 1.1538 - accuracy: 0.7600\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x1682aef70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x1682aef70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: nn_labelled_1000/assets\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x1695c33a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x1695c33a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "57/57 [==============================] - ETA: 0s - loss: 1.7657 - accuracy: 0.3893WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2bff2d700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2bff2d700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 1.7578 - accuracy: 0.3921 - val_loss: 0.8469 - val_accuracy: 0.7000\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.6821 - accuracy: 0.7647 - val_loss: 0.6731 - val_accuracy: 0.7650\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.6039 - accuracy: 0.7870 - val_loss: 0.6929 - val_accuracy: 0.7400\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.5318 - accuracy: 0.8212 - val_loss: 0.5939 - val_accuracy: 0.8000\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.4529 - accuracy: 0.8420 - val_loss: 0.5867 - val_accuracy: 0.8050\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.5047 - accuracy: 0.8294 - val_loss: 0.6670 - val_accuracy: 0.7500\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.4257 - accuracy: 0.8315 - val_loss: 0.5544 - val_accuracy: 0.8150\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.3333 - accuracy: 0.8862 - val_loss: 0.5769 - val_accuracy: 0.8150\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.2976 - accuracy: 0.8952 - val_loss: 0.6702 - val_accuracy: 0.7850\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.2798 - accuracy: 0.8956 - val_loss: 0.6554 - val_accuracy: 0.8000\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.3109 - accuracy: 0.8922 - val_loss: 0.6309 - val_accuracy: 0.8350\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.1882 - accuracy: 0.9276 - val_loss: 0.6162 - val_accuracy: 0.8000\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.1764 - accuracy: 0.9390 - val_loss: 0.7197 - val_accuracy: 0.8200\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.1796 - accuracy: 0.9428 - val_loss: 0.8340 - val_accuracy: 0.8200\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.2005 - accuracy: 0.9432 - val_loss: 0.7645 - val_accuracy: 0.8100\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.8955 - accuracy: 0.7853\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x1683861f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x1683861f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: nn_labelled_2000/assets\n",
      "peak memory: 1806.67 MiB, increment: 165.78 MiB\n",
      "CPU times: user 1min 22s, sys: 2.45 s, total: 1min 25s\n",
      "Wall time: 52.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "k=[500,1000,2000]\n",
    "model_2,history_2=np.empty(len(k),dtype=object),np.empty(len(k),dtype=object)\n",
    "for i in range(len(k)):\n",
    "    model_2[i]=get_model()\n",
    "    history_2[i]=train_model(model_2[i], X_train[:k[i]][..., np.newaxis], y_train[:k[i]])\n",
    "    model_2[i].evaluate(X_test[..., np.newaxis], y_test);\n",
    "    model_2[i].save('nn_labelled_{}'.format(k[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a7c704-92cc-421c-8193-eb11ddf09b01",
   "metadata": {},
   "source": [
    "# Clustering and Using Centroids as Representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f42c8e8-cbd9-44a6-aa8f-ae55f933df69",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x157de60d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x157de60d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-17 11:08:23.291330: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-07-17 11:08:23.293266: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - ETA: 0s - loss: 2.3030 - accuracy: 0.1300WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x15ea6e820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x15ea6e820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "15/15 [==============================] - 1s 32ms/step - loss: 2.3029 - accuracy: 0.1306 - val_loss: 2.2993 - val_accuracy: 0.1600\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2980 - accuracy: 0.1752 - val_loss: 2.2972 - val_accuracy: 0.1600\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2986 - accuracy: 0.1272 - val_loss: 2.2958 - val_accuracy: 0.1600\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2971 - accuracy: 0.1261 - val_loss: 2.2949 - val_accuracy: 0.1600\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2942 - accuracy: 0.1490 - val_loss: 2.2926 - val_accuracy: 0.1600\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2919 - accuracy: 0.1579 - val_loss: 2.2897 - val_accuracy: 0.1600\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2893 - accuracy: 0.1423 - val_loss: 2.2889 - val_accuracy: 0.1600\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2943 - accuracy: 0.1379 - val_loss: 2.2873 - val_accuracy: 0.1600\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2840 - accuracy: 0.1693 - val_loss: 2.2846 - val_accuracy: 0.1600\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2882 - accuracy: 0.1426 - val_loss: 2.2823 - val_accuracy: 0.1600\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2856 - accuracy: 0.1527 - val_loss: 2.2818 - val_accuracy: 0.1600\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2846 - accuracy: 0.1514 - val_loss: 2.2809 - val_accuracy: 0.1600\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2786 - accuracy: 0.1648 - val_loss: 2.2802 - val_accuracy: 0.1600\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2917 - accuracy: 0.1426 - val_loss: 2.2796 - val_accuracy: 0.1600\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2844 - accuracy: 0.1423 - val_loss: 2.2796 - val_accuracy: 0.1600\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2833 - accuracy: 0.1446 - val_loss: 2.2800 - val_accuracy: 0.1600\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2784 - accuracy: 0.1675 - val_loss: 2.2806 - val_accuracy: 0.1600\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2883 - accuracy: 0.1403 - val_loss: 2.2804 - val_accuracy: 0.1600\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2844 - accuracy: 0.1540 - val_loss: 2.2787 - val_accuracy: 0.1600\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2824 - accuracy: 0.1545 - val_loss: 2.2793 - val_accuracy: 0.1600\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2780 - accuracy: 0.1562 - val_loss: 2.2786 - val_accuracy: 0.1600\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2887 - accuracy: 0.1323 - val_loss: 2.2781 - val_accuracy: 0.1600\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2741 - accuracy: 0.1527 - val_loss: 2.2769 - val_accuracy: 0.1600\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2765 - accuracy: 0.1494 - val_loss: 2.2775 - val_accuracy: 0.1600\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2796 - accuracy: 0.1474 - val_loss: 2.2782 - val_accuracy: 0.1600\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2821 - accuracy: 0.1325 - val_loss: 2.2786 - val_accuracy: 0.1600\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2918 - accuracy: 0.1349 - val_loss: 2.2789 - val_accuracy: 0.1600\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2698 - accuracy: 0.1608 - val_loss: 2.2791 - val_accuracy: 0.1600\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2749 - accuracy: 0.1421 - val_loss: 2.2789 - val_accuracy: 0.1600\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2863 - accuracy: 0.1528 - val_loss: 2.2792 - val_accuracy: 0.1600\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 2.2804 - accuracy: 0.1293 - val_loss: 2.2793 - val_accuracy: 0.1600\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 2.3211 - accuracy: 0.1000\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x15efa7160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x15efa7160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-17 11:08:38.137924: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: nn_centroid_cluster_500/assets\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x15effe040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x15effe040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 1.8107 - accuracy: 0.4151WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x15ef9d5e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x15ef9d5e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "29/29 [==============================] - 1s 28ms/step - loss: 1.7911 - accuracy: 0.4214 - val_loss: 0.9084 - val_accuracy: 0.6600\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.9242 - accuracy: 0.6723 - val_loss: 0.7547 - val_accuracy: 0.7300\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.7942 - accuracy: 0.7339 - val_loss: 1.1321 - val_accuracy: 0.6900\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.8331 - accuracy: 0.7364 - val_loss: 0.6897 - val_accuracy: 0.7300\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.6049 - accuracy: 0.7663 - val_loss: 0.7703 - val_accuracy: 0.6700\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.5263 - accuracy: 0.8148 - val_loss: 0.5705 - val_accuracy: 0.7500\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.4832 - accuracy: 0.8246 - val_loss: 0.5508 - val_accuracy: 0.7400\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.4146 - accuracy: 0.8490 - val_loss: 0.5685 - val_accuracy: 0.8400\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.4244 - accuracy: 0.8581 - val_loss: 0.5296 - val_accuracy: 0.8200\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.3413 - accuracy: 0.8802 - val_loss: 0.5503 - val_accuracy: 0.7900\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.5016 - accuracy: 0.8244 - val_loss: 0.5792 - val_accuracy: 0.8300\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.3235 - accuracy: 0.8940 - val_loss: 0.5869 - val_accuracy: 0.7800\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.2850 - accuracy: 0.8975 - val_loss: 0.7988 - val_accuracy: 0.6900\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.3623 - accuracy: 0.8538 - val_loss: 0.5454 - val_accuracy: 0.8200\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.2923 - accuracy: 0.8946 - val_loss: 0.6407 - val_accuracy: 0.8000\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.2724 - accuracy: 0.9181 - val_loss: 0.5914 - val_accuracy: 0.8200\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.3380 - accuracy: 0.8893 - val_loss: 0.7040 - val_accuracy: 0.7600\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.8657 - accuracy: 0.7657\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2b5466940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2b5466940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: nn_centroid_cluster_1000/assets\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b54abdc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b54abdc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "55/57 [===========================>..] - ETA: 0s - loss: 1.6875 - accuracy: 0.4046WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x15f960940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x15f960940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 1.6647 - accuracy: 0.4130 - val_loss: 0.8155 - val_accuracy: 0.7100\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.7893 - accuracy: 0.7366 - val_loss: 0.7264 - val_accuracy: 0.7200\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.5872 - accuracy: 0.7946 - val_loss: 0.6466 - val_accuracy: 0.7900\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.5473 - accuracy: 0.8108 - val_loss: 0.6319 - val_accuracy: 0.7550\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.4319 - accuracy: 0.8433 - val_loss: 0.5698 - val_accuracy: 0.7700\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.3653 - accuracy: 0.8655 - val_loss: 0.5119 - val_accuracy: 0.8050\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.3938 - accuracy: 0.8664 - val_loss: 0.5751 - val_accuracy: 0.7900\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.3465 - accuracy: 0.8737 - val_loss: 0.5129 - val_accuracy: 0.8050\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.2842 - accuracy: 0.8998 - val_loss: 0.5696 - val_accuracy: 0.7900\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.2856 - accuracy: 0.9003 - val_loss: 0.6148 - val_accuracy: 0.8000\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.3065 - accuracy: 0.8830 - val_loss: 0.5928 - val_accuracy: 0.8150\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.2592 - accuracy: 0.9042 - val_loss: 0.5803 - val_accuracy: 0.7900\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.2879 - accuracy: 0.8978 - val_loss: 0.5075 - val_accuracy: 0.8200\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.2148 - accuracy: 0.9188 - val_loss: 0.5516 - val_accuracy: 0.8050\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.2535 - accuracy: 0.9110 - val_loss: 0.5543 - val_accuracy: 0.8200\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.2359 - accuracy: 0.9146 - val_loss: 0.5196 - val_accuracy: 0.8300\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.1828 - accuracy: 0.9304 - val_loss: 0.7755 - val_accuracy: 0.8100\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.1746 - accuracy: 0.9297 - val_loss: 0.6935 - val_accuracy: 0.7950\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.1660 - accuracy: 0.9369 - val_loss: 0.6389 - val_accuracy: 0.8200\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.2278 - accuracy: 0.9144 - val_loss: 0.5964 - val_accuracy: 0.8250\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.1214 - accuracy: 0.9540 - val_loss: 0.6500 - val_accuracy: 0.8300\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.8665 - accuracy: 0.8024\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x157d698b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x157d698b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: nn_centroid_cluster_2000/assets\n",
      "peak memory: 3341.53 MiB, increment: 2227.14 MiB\n",
      "CPU times: user 3h 21min 53s, sys: 37min 59s, total: 3h 59min 52s\n",
      "Wall time: 36min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "k=[500,1000,2000]\n",
    "kmeans_2=np.empty(len(k),dtype=object)\n",
    "model_3,history_3=np.empty(len(k),dtype=object),np.empty(len(k),dtype=object)\n",
    "for i in range(len(k)):\n",
    "    kmeans_2[i] = KMeans(init='k-means++',n_clusters = k[i], random_state = seed)\n",
    "    X_items_dist = kmeans_2[i].fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    representative_items_idx = np.argmin(X_items_dist, axis=0)\n",
    "    X_representative_items = X_train[representative_items_idx]\n",
    "    y_representative_items = y_train[representative_items_idx]\n",
    "    \n",
    "    model_3[i]=get_model()\n",
    "    history_3[i]= train_model(model_3[i], X_representative_items[..., np.newaxis], y_representative_items)\n",
    "    model_3[i].evaluate(X_test[..., np.newaxis], y_test);\n",
    "    model_3[i].save('nn_centroid_cluster_{}'.format(k[i]))\n",
    "    dump(kmeans_2[i],'cluster_nn_{}.joblib'.format(k[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e87be3e-acb5-49ee-8c8d-85ca5da65dfc",
   "metadata": {},
   "source": [
    "## Clustering and Propgating Labels to each Data Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db265756-3959-4701-bcf5-3952010823a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_label_prop(k=500):\n",
    "    kmeans=load('cluster_nn_{}.joblib'.format(k))\n",
    "    X_items_dist=kmeans.transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    representative_items_idx = np.argmin(X_items_dist, axis=0)\n",
    "    X_representative_items = X_train[representative_items_idx]\n",
    "    y_representative_items = y_train[representative_items_idx]\n",
    "    y_train_propagated = np.empty(len(X_train), dtype=np.int32)\n",
    "    for i in range(k):\n",
    "        y_train_propagated[kmeans.labels_==i] = y_representative_items[i]\n",
    "    \n",
    "    #construct and train fresh neural network\n",
    "    model_4=get_model()\n",
    "    history_4 = train_model(model_4, X_train[..., np.newaxis], y_train_propagated)\n",
    "    model_4.evaluate(X_test[..., np.newaxis], y_test);\n",
    "    model_4.save('nn_full_propagated_{}'.format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a348f5b4-58ad-41d6-a133-29faad3a13b2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters:  500\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b551f820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b551f820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "426/427 [============================>.] - ETA: 0s - loss: 0.8639 - accuracy: 0.6846WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2b551ff70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2b551ff70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "427/427 [==============================] - 11s 26ms/step - loss: 0.8627 - accuracy: 0.6850 - val_loss: 0.4386 - val_accuracy: 0.8386\n",
      "Epoch 2/100\n",
      "427/427 [==============================] - 11s 26ms/step - loss: 0.4181 - accuracy: 0.8423 - val_loss: 0.3865 - val_accuracy: 0.8524\n",
      "Epoch 3/100\n",
      "427/427 [==============================] - 11s 25ms/step - loss: 0.3626 - accuracy: 0.8607 - val_loss: 0.3296 - val_accuracy: 0.8781\n",
      "Epoch 4/100\n",
      "427/427 [==============================] - 10s 24ms/step - loss: 0.3323 - accuracy: 0.8708 - val_loss: 0.3203 - val_accuracy: 0.8735\n",
      "Epoch 5/100\n",
      "427/427 [==============================] - 10s 24ms/step - loss: 0.3025 - accuracy: 0.8826 - val_loss: 0.3245 - val_accuracy: 0.8814\n",
      "Epoch 6/100\n",
      "427/427 [==============================] - 10s 24ms/step - loss: 0.2814 - accuracy: 0.8886 - val_loss: 0.3035 - val_accuracy: 0.8854\n",
      "Epoch 7/100\n",
      "427/427 [==============================] - 10s 24ms/step - loss: 0.2660 - accuracy: 0.8970 - val_loss: 0.2816 - val_accuracy: 0.8972\n",
      "Epoch 8/100\n",
      "427/427 [==============================] - 11s 25ms/step - loss: 0.2359 - accuracy: 0.9041 - val_loss: 0.3219 - val_accuracy: 0.8867\n",
      "Epoch 9/100\n",
      "427/427 [==============================] - 11s 25ms/step - loss: 0.2228 - accuracy: 0.9082 - val_loss: 0.2825 - val_accuracy: 0.8959\n",
      "Epoch 10/100\n",
      "427/427 [==============================] - 11s 25ms/step - loss: 0.2000 - accuracy: 0.9167 - val_loss: 0.2941 - val_accuracy: 0.8874\n",
      "Epoch 11/100\n",
      "427/427 [==============================] - 10s 25ms/step - loss: 0.1898 - accuracy: 0.9249 - val_loss: 0.2811 - val_accuracy: 0.8979\n",
      "Epoch 12/100\n",
      "427/427 [==============================] - 11s 25ms/step - loss: 0.1720 - accuracy: 0.9286 - val_loss: 0.2727 - val_accuracy: 0.8986\n",
      "Epoch 13/100\n",
      "427/427 [==============================] - 11s 25ms/step - loss: 0.1566 - accuracy: 0.9374 - val_loss: 0.3216 - val_accuracy: 0.8841\n",
      "Epoch 14/100\n",
      "427/427 [==============================] - 11s 26ms/step - loss: 0.1416 - accuracy: 0.9423 - val_loss: 0.2973 - val_accuracy: 0.9025\n",
      "Epoch 15/100\n",
      "427/427 [==============================] - 11s 25ms/step - loss: 0.1215 - accuracy: 0.9525 - val_loss: 0.2791 - val_accuracy: 0.9012\n",
      "Epoch 16/100\n",
      "427/427 [==============================] - 11s 25ms/step - loss: 0.1054 - accuracy: 0.9593 - val_loss: 0.3005 - val_accuracy: 0.8972\n",
      "Epoch 17/100\n",
      "427/427 [==============================] - 10s 25ms/step - loss: 0.1070 - accuracy: 0.9570 - val_loss: 0.3442 - val_accuracy: 0.8906\n",
      "Epoch 18/100\n",
      "427/427 [==============================] - 11s 25ms/step - loss: 0.0988 - accuracy: 0.9647 - val_loss: 0.3712 - val_accuracy: 0.8913\n",
      "Epoch 19/100\n",
      "427/427 [==============================] - 11s 26ms/step - loss: 0.0839 - accuracy: 0.9662 - val_loss: 0.3263 - val_accuracy: 0.9065\n",
      "Epoch 20/100\n",
      "427/427 [==============================] - 11s 26ms/step - loss: 0.0744 - accuracy: 0.9728 - val_loss: 0.3736 - val_accuracy: 0.9038\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 1.9927 - accuracy: 0.7444\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2c1a9dd30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2c1a9dd30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: nn_partially_propagated_500/assets\n",
      "Clusters:  1000\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x15f960c10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x15f960c10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "432/433 [============================>.] - ETA: 0s - loss: 0.8554 - accuracy: 0.6855WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2b9ec6430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2b9ec6430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "433/433 [==============================] - 11s 25ms/step - loss: 0.8543 - accuracy: 0.6860 - val_loss: 0.4836 - val_accuracy: 0.8263\n",
      "Epoch 2/100\n",
      "433/433 [==============================] - 11s 24ms/step - loss: 0.4508 - accuracy: 0.8342 - val_loss: 0.4091 - val_accuracy: 0.8367\n",
      "Epoch 3/100\n",
      "433/433 [==============================] - 11s 24ms/step - loss: 0.3986 - accuracy: 0.8520 - val_loss: 0.4123 - val_accuracy: 0.8373\n",
      "Epoch 4/100\n",
      "433/433 [==============================] - 11s 24ms/step - loss: 0.3586 - accuracy: 0.8667 - val_loss: 0.3888 - val_accuracy: 0.8484\n",
      "Epoch 5/100\n",
      "433/433 [==============================] - 11s 24ms/step - loss: 0.3444 - accuracy: 0.8736 - val_loss: 0.3725 - val_accuracy: 0.8640\n",
      "Epoch 6/100\n",
      "433/433 [==============================] - 11s 24ms/step - loss: 0.3186 - accuracy: 0.8770 - val_loss: 0.3202 - val_accuracy: 0.8777\n",
      "Epoch 7/100\n",
      "433/433 [==============================] - 11s 24ms/step - loss: 0.2884 - accuracy: 0.8942 - val_loss: 0.3515 - val_accuracy: 0.8699\n",
      "Epoch 8/100\n",
      "433/433 [==============================] - 11s 26ms/step - loss: 0.2622 - accuracy: 0.8997 - val_loss: 0.3230 - val_accuracy: 0.8783\n",
      "Epoch 9/100\n",
      "433/433 [==============================] - 11s 25ms/step - loss: 0.2459 - accuracy: 0.9063 - val_loss: 0.3112 - val_accuracy: 0.8861\n",
      "Epoch 10/100\n",
      "433/433 [==============================] - 11s 25ms/step - loss: 0.2409 - accuracy: 0.9070 - val_loss: 0.3447 - val_accuracy: 0.8835\n",
      "Epoch 11/100\n",
      "433/433 [==============================] - 11s 25ms/step - loss: 0.2304 - accuracy: 0.9122 - val_loss: 0.3104 - val_accuracy: 0.8842\n",
      "Epoch 12/100\n",
      "433/433 [==============================] - 11s 25ms/step - loss: 0.2047 - accuracy: 0.9230 - val_loss: 0.3402 - val_accuracy: 0.8855\n",
      "Epoch 13/100\n",
      "433/433 [==============================] - 11s 25ms/step - loss: 0.1864 - accuracy: 0.9277 - val_loss: 0.3315 - val_accuracy: 0.8803\n",
      "Epoch 14/100\n",
      "433/433 [==============================] - 11s 25ms/step - loss: 0.1757 - accuracy: 0.9324 - val_loss: 0.3199 - val_accuracy: 0.8939\n",
      "Epoch 15/100\n",
      "433/433 [==============================] - 11s 25ms/step - loss: 0.1596 - accuracy: 0.9378 - val_loss: 0.3619 - val_accuracy: 0.8783\n",
      "Epoch 16/100\n",
      "433/433 [==============================] - 11s 25ms/step - loss: 0.1601 - accuracy: 0.9380 - val_loss: 0.3541 - val_accuracy: 0.8894\n",
      "Epoch 17/100\n",
      "433/433 [==============================] - 11s 25ms/step - loss: 0.1521 - accuracy: 0.9408 - val_loss: 0.4454 - val_accuracy: 0.8803\n",
      "Epoch 18/100\n",
      "433/433 [==============================] - 11s 25ms/step - loss: 0.1341 - accuracy: 0.9463 - val_loss: 0.4155 - val_accuracy: 0.8868\n",
      "Epoch 19/100\n",
      "433/433 [==============================] - 11s 25ms/step - loss: 0.1204 - accuracy: 0.9539 - val_loss: 0.4115 - val_accuracy: 0.8887\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 1.4131 - accuracy: 0.7596\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2b9e78550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2b9e78550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: nn_partially_propagated_1000/assets\n",
      "Clusters:  2000\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2c1a5eca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2c1a5eca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "443/444 [============================>.] - ETA: 0s - loss: 0.9047 - accuracy: 0.6778WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2c1a5e790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2c1a5e790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "444/444 [==============================] - 12s 27ms/step - loss: 0.9034 - accuracy: 0.6783 - val_loss: 0.3803 - val_accuracy: 0.8668\n",
      "Epoch 2/100\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.4048 - accuracy: 0.8534 - val_loss: 0.3582 - val_accuracy: 0.8744\n",
      "Epoch 3/100\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.3638 - accuracy: 0.8638 - val_loss: 0.3447 - val_accuracy: 0.8782\n",
      "Epoch 4/100\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.3126 - accuracy: 0.8780 - val_loss: 0.3143 - val_accuracy: 0.8896\n",
      "Epoch 5/100\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.2921 - accuracy: 0.8889 - val_loss: 0.3418 - val_accuracy: 0.8794\n",
      "Epoch 6/100\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.2727 - accuracy: 0.8952 - val_loss: 0.3340 - val_accuracy: 0.8820\n",
      "Epoch 7/100\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.2641 - accuracy: 0.8966 - val_loss: 0.3163 - val_accuracy: 0.8896\n",
      "Epoch 8/100\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.2360 - accuracy: 0.9067 - val_loss: 0.3089 - val_accuracy: 0.8915\n",
      "Epoch 9/100\n",
      "444/444 [==============================] - 12s 26ms/step - loss: 0.2220 - accuracy: 0.9124 - val_loss: 0.3386 - val_accuracy: 0.8896\n",
      "Epoch 10/100\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.2062 - accuracy: 0.9233 - val_loss: 0.3410 - val_accuracy: 0.8852\n",
      "Epoch 11/100\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.1969 - accuracy: 0.9256 - val_loss: 0.3673 - val_accuracy: 0.8820\n",
      "Epoch 12/100\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.1677 - accuracy: 0.9364 - val_loss: 0.3900 - val_accuracy: 0.8883\n",
      "Epoch 13/100\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.1533 - accuracy: 0.9402 - val_loss: 0.4253 - val_accuracy: 0.8750\n",
      "Epoch 14/100\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.1367 - accuracy: 0.9455 - val_loss: 0.4319 - val_accuracy: 0.8706\n",
      "Epoch 15/100\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.1250 - accuracy: 0.9504 - val_loss: 0.4226 - val_accuracy: 0.8782\n",
      "Epoch 16/100\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.1116 - accuracy: 0.9574 - val_loss: 0.4523 - val_accuracy: 0.8826\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 1.1310 - accuracy: 0.7803\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x15f558040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x15f558040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: nn_partially_propagated_2000/assets\n",
      "peak memory: 3275.19 MiB, increment: 1604.97 MiB\n",
      "CPU times: user 17min 1s, sys: 25.3 s, total: 17min 26s\n",
      "Wall time: 10min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "for i in k:\n",
    "    print('Clusters: ',i)\n",
    "    cluster_label_prop(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d72ff-4465-4288-b8f0-63223015c79f",
   "metadata": {},
   "source": [
    "## Clustering and Propgating Labels to Data Points Closer to Cluster Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99cc8cdb-6125-469c-9ac5-e964b6a0ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_label_prop(k=500,percentile_closest=25):\n",
    "    kmeans=load('cluster_nn_{}.joblib'.format(k))\n",
    "    X_items_dist=kmeans.transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_cluster_dist = X_items_dist[np.arange(len(X_train)), kmeans.labels_]\n",
    "    representative_items_idx = np.argmin(X_items_dist, axis=0)\n",
    "    y_representative_items = y_train[representative_items_idx]\n",
    "    y_train_propagated = np.empty(len(X_train), dtype=np.int32)\n",
    "    for i in range(k):\n",
    "        in_cluster = (kmeans.labels_ == i)\n",
    "        cluster_dist = X_cluster_dist[in_cluster]\n",
    "        cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
    "        above_cutoff = (X_cluster_dist > cutoff_distance)\n",
    "        X_cluster_dist[in_cluster & above_cutoff] = -1\n",
    "        y_train_propagated[kmeans.labels_==i] = y_representative_items[i]\n",
    "        \n",
    "    # Allocating Labels to Marked Point in Cluster\n",
    "    partially_propagated = (X_cluster_dist != -1)\n",
    "    X_train_partially_propagated = X_train[partially_propagated]\n",
    "    y_train_partially_propagated = y_train_propagated[partially_propagated]\n",
    "    \n",
    "    #construct and train fresh neural network\n",
    "    model_5=get_model()\n",
    "    history_5 = train_model(model_5, X_train_partially_propagated[..., np.newaxis], y_train_partially_propagated)\n",
    "    model_5.evaluate(X_test[..., np.newaxis], y_test);\n",
    "    model_5.save('nn_partially_propagated_{}-clusters_{}-percentile'.format(k,percentile_closest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a01b823-a76f-43cb-a6b2-5f7f0d2d7471",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters:  500\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x15f9609d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x15f9609d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "427/427 [==============================] - ETA: 0s - loss: 2.2991 - accuracy: 0.1121WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2c14d59d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2c14d59d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "427/427 [==============================] - 11s 25ms/step - loss: 2.2991 - accuracy: 0.1121 - val_loss: 2.2943 - val_accuracy: 0.1192\n",
      "Epoch 2/100\n",
      "427/427 [==============================] - 10s 24ms/step - loss: 2.2928 - accuracy: 0.1269 - val_loss: 2.2935 - val_accuracy: 0.1192\n",
      "Epoch 3/100\n",
      "427/427 [==============================] - 10s 24ms/step - loss: 2.2948 - accuracy: 0.1264 - val_loss: 2.2929 - val_accuracy: 0.1192\n",
      "Epoch 4/100\n",
      "427/427 [==============================] - 10s 24ms/step - loss: 2.2939 - accuracy: 0.1253 - val_loss: 2.2932 - val_accuracy: 0.1192\n",
      "Epoch 5/100\n",
      "427/427 [==============================] - 10s 24ms/step - loss: 2.2940 - accuracy: 0.1269 - val_loss: 2.2932 - val_accuracy: 0.1192\n",
      "Epoch 6/100\n",
      "427/427 [==============================] - 10s 24ms/step - loss: 2.2944 - accuracy: 0.1240 - val_loss: 2.2932 - val_accuracy: 0.1192\n",
      "Epoch 7/100\n",
      "427/427 [==============================] - 10s 24ms/step - loss: 2.2946 - accuracy: 0.1240 - val_loss: 2.2928 - val_accuracy: 0.1192\n",
      "Epoch 8/100\n",
      "427/427 [==============================] - 10s 24ms/step - loss: 2.2955 - accuracy: 0.1239 - val_loss: 2.2931 - val_accuracy: 0.1192\n",
      "Epoch 9/100\n",
      "427/427 [==============================] - 10s 24ms/step - loss: 2.2975 - accuracy: 0.1213 - val_loss: 2.2935 - val_accuracy: 0.1192\n",
      "Epoch 10/100\n",
      "427/427 [==============================] - 10s 25ms/step - loss: 2.2930 - accuracy: 0.1257 - val_loss: 2.2935 - val_accuracy: 0.1192\n",
      "Epoch 11/100\n",
      "427/427 [==============================] - 10s 24ms/step - loss: 2.2942 - accuracy: 0.1230 - val_loss: 2.2935 - val_accuracy: 0.1192\n",
      "Epoch 12/100\n",
      "427/427 [==============================] - 10s 24ms/step - loss: 2.2942 - accuracy: 0.1229 - val_loss: 2.2930 - val_accuracy: 0.1192\n",
      "Epoch 13/100\n",
      "427/427 [==============================] - 10s 24ms/step - loss: 2.2966 - accuracy: 0.1208 - val_loss: 2.2932 - val_accuracy: 0.1192\n",
      "Epoch 14/100\n",
      "427/427 [==============================] - 10s 24ms/step - loss: 2.2964 - accuracy: 0.1183 - val_loss: 2.2929 - val_accuracy: 0.1192\n",
      "Epoch 15/100\n",
      "427/427 [==============================] - 10s 24ms/step - loss: 2.2949 - accuracy: 0.1225 - val_loss: 2.2931 - val_accuracy: 0.1192\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 2.3116 - accuracy: 0.1000\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2b997f700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2b997f700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: nn_partially_propagated_500/assets\n",
      "Clusters:  1000\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b551f940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b551f940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "432/433 [============================>.] - ETA: 0s - loss: 2.2998 - accuracy: 0.1090WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2b551f430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2b551f430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "433/433 [==============================] - 11s 24ms/step - loss: 2.2997 - accuracy: 0.1090 - val_loss: 2.2975 - val_accuracy: 0.1152\n",
      "Epoch 2/100\n",
      "433/433 [==============================] - 10s 24ms/step - loss: 2.2961 - accuracy: 0.1118 - val_loss: 2.2975 - val_accuracy: 0.1152\n",
      "Epoch 3/100\n",
      "433/433 [==============================] - 10s 24ms/step - loss: 2.2950 - accuracy: 0.1121 - val_loss: 2.2976 - val_accuracy: 0.1041\n",
      "Epoch 4/100\n",
      "433/433 [==============================] - 10s 24ms/step - loss: 2.2963 - accuracy: 0.1094 - val_loss: 2.2974 - val_accuracy: 0.1152\n",
      "Epoch 5/100\n",
      "433/433 [==============================] - 10s 24ms/step - loss: 2.2966 - accuracy: 0.1138 - val_loss: 2.2975 - val_accuracy: 0.1152\n",
      "Epoch 6/100\n",
      "433/433 [==============================] - 10s 24ms/step - loss: 2.2941 - accuracy: 0.1150 - val_loss: 2.2974 - val_accuracy: 0.1041\n",
      "Epoch 7/100\n",
      "433/433 [==============================] - 10s 24ms/step - loss: 2.2943 - accuracy: 0.1180 - val_loss: 2.2979 - val_accuracy: 0.1041\n",
      "Epoch 8/100\n",
      "433/433 [==============================] - 10s 24ms/step - loss: 2.2961 - accuracy: 0.1105 - val_loss: 2.2974 - val_accuracy: 0.1041\n",
      "Epoch 9/100\n",
      "433/433 [==============================] - 10s 24ms/step - loss: 2.2952 - accuracy: 0.1099 - val_loss: 2.2973 - val_accuracy: 0.1041\n",
      "Epoch 10/100\n",
      "433/433 [==============================] - 10s 24ms/step - loss: 2.2959 - accuracy: 0.1145 - val_loss: 2.2974 - val_accuracy: 0.1041\n",
      "Epoch 11/100\n",
      "433/433 [==============================] - 10s 24ms/step - loss: 2.2944 - accuracy: 0.1173 - val_loss: 2.2974 - val_accuracy: 0.1087\n",
      "Epoch 12/100\n",
      "433/433 [==============================] - 11s 24ms/step - loss: 2.2961 - accuracy: 0.1106 - val_loss: 2.2974 - val_accuracy: 0.1087\n",
      "Epoch 13/100\n",
      "433/433 [==============================] - 11s 24ms/step - loss: 2.2956 - accuracy: 0.1095 - val_loss: 2.2972 - val_accuracy: 0.1152\n",
      "Epoch 14/100\n",
      "433/433 [==============================] - 10s 24ms/step - loss: 2.2967 - accuracy: 0.1072 - val_loss: 2.2977 - val_accuracy: 0.1041\n",
      "Epoch 15/100\n",
      "433/433 [==============================] - 10s 24ms/step - loss: 2.2952 - accuracy: 0.1126 - val_loss: 2.2973 - val_accuracy: 0.1041\n",
      "Epoch 16/100\n",
      "433/433 [==============================] - 11s 24ms/step - loss: 2.2955 - accuracy: 0.1142 - val_loss: 2.2971 - val_accuracy: 0.1041\n",
      "Epoch 17/100\n",
      "433/433 [==============================] - 10s 24ms/step - loss: 2.2964 - accuracy: 0.1077 - val_loss: 2.2972 - val_accuracy: 0.1041\n",
      "Epoch 18/100\n",
      "433/433 [==============================] - 10s 24ms/step - loss: 2.2960 - accuracy: 0.1140 - val_loss: 2.2973 - val_accuracy: 0.1041\n",
      "Epoch 19/100\n",
      "433/433 [==============================] - 10s 24ms/step - loss: 2.2956 - accuracy: 0.1150 - val_loss: 2.2973 - val_accuracy: 0.1041\n",
      "Epoch 20/100\n",
      "433/433 [==============================] - 10s 24ms/step - loss: 2.2953 - accuracy: 0.1132 - val_loss: 2.2974 - val_accuracy: 0.1041\n",
      "Epoch 21/100\n",
      "433/433 [==============================] - 11s 25ms/step - loss: 2.2952 - accuracy: 0.1089 - val_loss: 2.2975 - val_accuracy: 0.1041\n",
      "Epoch 22/100\n",
      "433/433 [==============================] - 10s 24ms/step - loss: 2.2967 - accuracy: 0.1127 - val_loss: 2.2974 - val_accuracy: 0.1041\n",
      "Epoch 23/100\n",
      "433/433 [==============================] - 10s 24ms/step - loss: 2.2960 - accuracy: 0.1128 - val_loss: 2.2973 - val_accuracy: 0.1041\n",
      "Epoch 24/100\n",
      "433/433 [==============================] - 10s 24ms/step - loss: 2.2953 - accuracy: 0.1145 - val_loss: 2.2975 - val_accuracy: 0.1041\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 2.3092 - accuracy: 0.1000\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2bb49f9d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2bb49f9d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: nn_partially_propagated_1000/assets\n",
      "Clusters:  2000\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x15f9e3040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x15f9e3040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "444/444 [==============================] - ETA: 0s - loss: 0.9072 - accuracy: 0.6651WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2b5476820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2b5476820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.9066 - accuracy: 0.6653 - val_loss: 0.4325 - val_accuracy: 0.8471\n",
      "Epoch 2/100\n",
      "444/444 [==============================] - 11s 24ms/step - loss: 0.4432 - accuracy: 0.8360 - val_loss: 0.4189 - val_accuracy: 0.8477\n",
      "Epoch 3/100\n",
      "444/444 [==============================] - 11s 26ms/step - loss: 0.3756 - accuracy: 0.8589 - val_loss: 0.3750 - val_accuracy: 0.8718\n",
      "Epoch 4/100\n",
      "444/444 [==============================] - 11s 26ms/step - loss: 0.3606 - accuracy: 0.8623 - val_loss: 0.3476 - val_accuracy: 0.8737\n",
      "Epoch 5/100\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.3162 - accuracy: 0.8772 - val_loss: 0.3331 - val_accuracy: 0.8807\n",
      "Epoch 6/100\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.2978 - accuracy: 0.8857 - val_loss: 0.3400 - val_accuracy: 0.8813\n",
      "Epoch 7/100\n",
      "444/444 [==============================] - 11s 24ms/step - loss: 0.2888 - accuracy: 0.8887 - val_loss: 0.3287 - val_accuracy: 0.8845\n",
      "Epoch 8/100\n",
      "444/444 [==============================] - 11s 24ms/step - loss: 0.2628 - accuracy: 0.8946 - val_loss: 0.3393 - val_accuracy: 0.8788\n",
      "Epoch 9/100\n",
      "444/444 [==============================] - 11s 24ms/step - loss: 0.2460 - accuracy: 0.9064 - val_loss: 0.3324 - val_accuracy: 0.8845\n",
      "Epoch 10/100\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.2297 - accuracy: 0.9091 - val_loss: 0.3621 - val_accuracy: 0.8871\n",
      "Epoch 11/100\n",
      "444/444 [==============================] - 11s 24ms/step - loss: 0.2256 - accuracy: 0.9129 - val_loss: 0.3389 - val_accuracy: 0.8959\n",
      "Epoch 12/100\n",
      "444/444 [==============================] - 11s 24ms/step - loss: 0.2038 - accuracy: 0.9193 - val_loss: 0.3410 - val_accuracy: 0.8864\n",
      "Epoch 13/100\n",
      "444/444 [==============================] - 11s 24ms/step - loss: 0.1995 - accuracy: 0.9213 - val_loss: 0.3570 - val_accuracy: 0.8832\n",
      "Epoch 14/100\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.1859 - accuracy: 0.9256 - val_loss: 0.3677 - val_accuracy: 0.8915\n",
      "Epoch 15/100\n",
      "444/444 [==============================] - 11s 25ms/step - loss: 0.1571 - accuracy: 0.9398 - val_loss: 0.3591 - val_accuracy: 0.8896\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.8659 - accuracy: 0.7942\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2b54584c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2b54584c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: nn_partially_propagated_2000/assets\n",
      "peak memory: 3051.58 MiB, increment: 1657.17 MiB\n",
      "CPU times: user 16min 16s, sys: 24.5 s, total: 16min 41s\n",
      "Wall time: 9min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "for i in k:\n",
    "    print('Clusters: ',i)\n",
    "    cluster_label_prop(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0720bd77-33f9-49ae-b662-af410eb471f3",
   "metadata": {},
   "source": [
    "## Taking 2000 clusters and determining optimum percentile for propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0dc0a76-8dbf-4736-b5e9-294fa4e1f4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentile=  20\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b997f700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b997f700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "360/361 [============================>.] - ETA: 0s - loss: 2.3013 - accuracy: 0.1034WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2c14d5430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2c14d5430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "361/361 [==============================] - 9s 25ms/step - loss: 2.3013 - accuracy: 0.1034 - val_loss: 2.2945 - val_accuracy: 0.1132\n",
      "Epoch 2/100\n",
      "361/361 [==============================] - 9s 25ms/step - loss: 2.2989 - accuracy: 0.1097 - val_loss: 2.2931 - val_accuracy: 0.1397\n",
      "Epoch 3/100\n",
      "361/361 [==============================] - 9s 25ms/step - loss: 2.2998 - accuracy: 0.1069 - val_loss: 2.2924 - val_accuracy: 0.1054\n",
      "Epoch 4/100\n",
      "361/361 [==============================] - 10s 28ms/step - loss: 2.2997 - accuracy: 0.1049 - val_loss: 2.2925 - val_accuracy: 0.1116\n",
      "Epoch 5/100\n",
      "361/361 [==============================] - 9s 26ms/step - loss: 2.2989 - accuracy: 0.1084 - val_loss: 2.2925 - val_accuracy: 0.1116\n",
      "Epoch 6/100\n",
      "361/361 [==============================] - 9s 25ms/step - loss: 2.2997 - accuracy: 0.1124 - val_loss: 2.2925 - val_accuracy: 0.1054\n",
      "Epoch 7/100\n",
      "361/361 [==============================] - 9s 24ms/step - loss: 2.2995 - accuracy: 0.1103 - val_loss: 2.2923 - val_accuracy: 0.1054\n",
      "Epoch 8/100\n",
      "361/361 [==============================] - 9s 25ms/step - loss: 2.3002 - accuracy: 0.1099 - val_loss: 2.2916 - val_accuracy: 0.1132\n",
      "Epoch 9/100\n",
      "361/361 [==============================] - 10s 27ms/step - loss: 2.2975 - accuracy: 0.1085 - val_loss: 2.2927 - val_accuracy: 0.1116\n",
      "Epoch 10/100\n",
      "361/361 [==============================] - 9s 26ms/step - loss: 2.2997 - accuracy: 0.1051 - val_loss: 2.2920 - val_accuracy: 0.1054\n",
      "Epoch 11/100\n",
      "361/361 [==============================] - 10s 27ms/step - loss: 2.3009 - accuracy: 0.1043 - val_loss: 2.2920 - val_accuracy: 0.1054\n",
      "Epoch 12/100\n",
      "361/361 [==============================] - 9s 25ms/step - loss: 2.3003 - accuracy: 0.1029 - val_loss: 2.2920 - val_accuracy: 0.1054\n",
      "Epoch 13/100\n",
      "361/361 [==============================] - 9s 25ms/step - loss: 2.3008 - accuracy: 0.1042 - val_loss: 2.2918 - val_accuracy: 0.1054\n",
      "Epoch 14/100\n",
      "361/361 [==============================] - 9s 25ms/step - loss: 2.2989 - accuracy: 0.1082 - val_loss: 2.2923 - val_accuracy: 0.1397\n",
      "Epoch 15/100\n",
      "361/361 [==============================] - 9s 25ms/step - loss: 2.3001 - accuracy: 0.1071 - val_loss: 2.2921 - val_accuracy: 0.1054\n",
      "Epoch 16/100\n",
      "361/361 [==============================] - 9s 26ms/step - loss: 2.2998 - accuracy: 0.1084 - val_loss: 2.2919 - val_accuracy: 0.1054\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 2.3063 - accuracy: 0.1000\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2c1a9dc10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2c1a9dc10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: nn_partially_propagated_2000-clusters_20-percentile/assets\n",
      "Percentile=  25\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x15ea6e0d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x15ea6e0d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "443/444 [============================>.] - ETA: 0s - loss: 2.3010 - accuracy: 0.1104WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2b9ec6a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2b9ec6a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "444/444 [==============================] - 12s 27ms/step - loss: 2.3010 - accuracy: 0.1104 - val_loss: 2.2947 - val_accuracy: 0.1326\n",
      "Epoch 2/100\n",
      "444/444 [==============================] - 12s 26ms/step - loss: 2.2994 - accuracy: 0.1103 - val_loss: 2.2947 - val_accuracy: 0.1326\n",
      "Epoch 3/100\n",
      "444/444 [==============================] - 12s 27ms/step - loss: 2.2988 - accuracy: 0.1066 - val_loss: 2.2944 - val_accuracy: 0.1060\n",
      "Epoch 4/100\n",
      "444/444 [==============================] - 12s 27ms/step - loss: 2.2984 - accuracy: 0.1110 - val_loss: 2.2947 - val_accuracy: 0.1326\n",
      "Epoch 5/100\n",
      "444/444 [==============================] - 12s 27ms/step - loss: 2.2997 - accuracy: 0.1126 - val_loss: 2.2940 - val_accuracy: 0.1326\n",
      "Epoch 6/100\n",
      "444/444 [==============================] - 12s 27ms/step - loss: 2.2996 - accuracy: 0.1012 - val_loss: 2.2938 - val_accuracy: 0.1326\n",
      "Epoch 7/100\n",
      "444/444 [==============================] - 12s 28ms/step - loss: 2.2985 - accuracy: 0.1088 - val_loss: 2.2948 - val_accuracy: 0.1326\n",
      "Epoch 8/100\n",
      "444/444 [==============================] - 12s 28ms/step - loss: 2.2993 - accuracy: 0.1110 - val_loss: 2.2942 - val_accuracy: 0.1326\n",
      "Epoch 9/100\n",
      "444/444 [==============================] - 12s 27ms/step - loss: 2.2993 - accuracy: 0.1096 - val_loss: 2.2940 - val_accuracy: 0.1326\n",
      "Epoch 10/100\n",
      "444/444 [==============================] - 12s 27ms/step - loss: 2.2995 - accuracy: 0.1070 - val_loss: 2.2938 - val_accuracy: 0.1326\n",
      "Epoch 11/100\n",
      "444/444 [==============================] - 12s 27ms/step - loss: 2.2983 - accuracy: 0.1074 - val_loss: 2.2948 - val_accuracy: 0.1060\n",
      "Epoch 12/100\n",
      "444/444 [==============================] - 12s 27ms/step - loss: 2.2996 - accuracy: 0.1047 - val_loss: 2.2941 - val_accuracy: 0.1110\n",
      "Epoch 13/100\n",
      "444/444 [==============================] - 12s 27ms/step - loss: 2.2984 - accuracy: 0.1150 - val_loss: 2.2938 - val_accuracy: 0.1326\n",
      "Epoch 14/100\n",
      "444/444 [==============================] - 12s 27ms/step - loss: 2.2976 - accuracy: 0.1172 - val_loss: 2.2948 - val_accuracy: 0.1060\n",
      "Epoch 15/100\n",
      "444/444 [==============================] - 12s 27ms/step - loss: 2.2986 - accuracy: 0.1144 - val_loss: 2.2946 - val_accuracy: 0.1326\n",
      "Epoch 16/100\n",
      "444/444 [==============================] - 12s 27ms/step - loss: 2.2997 - accuracy: 0.1124 - val_loss: 2.2942 - val_accuracy: 0.1326\n",
      "Epoch 17/100\n",
      "444/444 [==============================] - 12s 27ms/step - loss: 2.2995 - accuracy: 0.1069 - val_loss: 2.2948 - val_accuracy: 0.1326\n",
      "Epoch 18/100\n",
      "444/444 [==============================] - 12s 27ms/step - loss: 2.2992 - accuracy: 0.1049 - val_loss: 2.2941 - val_accuracy: 0.1326\n",
      "Epoch 19/100\n",
      "444/444 [==============================] - 12s 27ms/step - loss: 2.2983 - accuracy: 0.1138 - val_loss: 2.2940 - val_accuracy: 0.1326\n",
      "Epoch 20/100\n",
      "444/444 [==============================] - 12s 27ms/step - loss: 2.2992 - accuracy: 0.1070 - val_loss: 2.2940 - val_accuracy: 0.1326\n",
      "Epoch 21/100\n",
      "444/444 [==============================] - 12s 27ms/step - loss: 2.2991 - accuracy: 0.1095 - val_loss: 2.2941 - val_accuracy: 0.1326\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 2.3064 - accuracy: 0.1000\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2b5501ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2b5501ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: nn_partially_propagated_2000-clusters_25-percentile/assets\n",
      "Percentile=  30\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2c1a9dee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2c1a9dee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "519/521 [============================>.] - ETA: 0s - loss: 0.9336 - accuracy: 0.6771WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2baab8430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2baab8430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "521/521 [==============================] - 14s 27ms/step - loss: 0.9320 - accuracy: 0.6776 - val_loss: 0.4047 - val_accuracy: 0.8579\n",
      "Epoch 2/100\n",
      "521/521 [==============================] - 14s 27ms/step - loss: 0.4380 - accuracy: 0.8374 - val_loss: 0.3704 - val_accuracy: 0.8671\n",
      "Epoch 3/100\n",
      "521/521 [==============================] - 14s 27ms/step - loss: 0.3930 - accuracy: 0.8537 - val_loss: 0.3684 - val_accuracy: 0.8741\n",
      "Epoch 4/100\n",
      "521/521 [==============================] - 14s 27ms/step - loss: 0.3560 - accuracy: 0.8662 - val_loss: 0.3603 - val_accuracy: 0.8768\n",
      "Epoch 5/100\n",
      "521/521 [==============================] - 14s 28ms/step - loss: 0.3296 - accuracy: 0.8726 - val_loss: 0.3319 - val_accuracy: 0.8871\n",
      "Epoch 6/100\n",
      "521/521 [==============================] - 14s 28ms/step - loss: 0.3138 - accuracy: 0.8780 - val_loss: 0.3447 - val_accuracy: 0.8822\n",
      "Epoch 7/100\n",
      "521/521 [==============================] - 14s 27ms/step - loss: 0.2897 - accuracy: 0.8875 - val_loss: 0.3520 - val_accuracy: 0.8752\n",
      "Epoch 8/100\n",
      "521/521 [==============================] - 14s 28ms/step - loss: 0.2692 - accuracy: 0.8910 - val_loss: 0.3368 - val_accuracy: 0.8914\n",
      "Epoch 9/100\n",
      "521/521 [==============================] - 14s 28ms/step - loss: 0.2529 - accuracy: 0.8996 - val_loss: 0.3302 - val_accuracy: 0.8855\n",
      "Epoch 10/100\n",
      "521/521 [==============================] - 14s 27ms/step - loss: 0.2393 - accuracy: 0.9067 - val_loss: 0.3324 - val_accuracy: 0.8903\n",
      "Epoch 11/100\n",
      "521/521 [==============================] - 14s 27ms/step - loss: 0.2140 - accuracy: 0.9153 - val_loss: 0.3562 - val_accuracy: 0.8887\n",
      "Epoch 12/100\n",
      "521/521 [==============================] - 14s 27ms/step - loss: 0.1977 - accuracy: 0.9228 - val_loss: 0.3466 - val_accuracy: 0.8860\n",
      "Epoch 13/100\n",
      "521/521 [==============================] - 14s 27ms/step - loss: 0.1856 - accuracy: 0.9286 - val_loss: 0.3548 - val_accuracy: 0.8822\n",
      "Epoch 14/100\n",
      "521/521 [==============================] - 14s 28ms/step - loss: 0.1759 - accuracy: 0.9315 - val_loss: 0.3795 - val_accuracy: 0.8833\n",
      "Epoch 15/100\n",
      "521/521 [==============================] - 14s 28ms/step - loss: 0.1666 - accuracy: 0.9380 - val_loss: 0.3925 - val_accuracy: 0.8920\n",
      "Epoch 16/100\n",
      "521/521 [==============================] - 14s 27ms/step - loss: 0.1614 - accuracy: 0.9349 - val_loss: 0.4468 - val_accuracy: 0.8822\n",
      "Epoch 17/100\n",
      "521/521 [==============================] - 14s 27ms/step - loss: 0.1340 - accuracy: 0.9482 - val_loss: 0.4170 - val_accuracy: 0.8903\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.9840 - accuracy: 0.8012\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2b9ec65e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2b9ec65e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: nn_partially_propagated_2000-clusters_30-percentile/assets\n",
      "Percentile=  50\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b54abca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2b54abca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "857/858 [============================>.] - ETA: 0s - loss: 0.7548 - accuracy: 0.7223WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x15effe4c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x15effe4c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "858/858 [==============================] - 24s 27ms/step - loss: 0.7544 - accuracy: 0.7225 - val_loss: 0.4289 - val_accuracy: 0.8423\n",
      "Epoch 2/100\n",
      "858/858 [==============================] - 23s 27ms/step - loss: 0.4218 - accuracy: 0.8439 - val_loss: 0.3626 - val_accuracy: 0.8702\n",
      "Epoch 3/100\n",
      "858/858 [==============================] - 23s 27ms/step - loss: 0.3723 - accuracy: 0.8614 - val_loss: 0.3614 - val_accuracy: 0.8633\n",
      "Epoch 4/100\n",
      "858/858 [==============================] - 23s 27ms/step - loss: 0.3497 - accuracy: 0.8655 - val_loss: 0.3478 - val_accuracy: 0.8699\n",
      "Epoch 5/100\n",
      "858/858 [==============================] - 23s 27ms/step - loss: 0.3304 - accuracy: 0.8695 - val_loss: 0.3270 - val_accuracy: 0.8758\n",
      "Epoch 6/100\n",
      "858/858 [==============================] - 23s 27ms/step - loss: 0.3084 - accuracy: 0.8793 - val_loss: 0.3146 - val_accuracy: 0.8833\n",
      "Epoch 7/100\n",
      "858/858 [==============================] - 23s 27ms/step - loss: 0.2906 - accuracy: 0.8834 - val_loss: 0.3237 - val_accuracy: 0.8804\n",
      "Epoch 8/100\n",
      "858/858 [==============================] - 23s 27ms/step - loss: 0.2726 - accuracy: 0.8905 - val_loss: 0.3263 - val_accuracy: 0.8833\n",
      "Epoch 9/100\n",
      "858/858 [==============================] - 23s 27ms/step - loss: 0.2501 - accuracy: 0.8976 - val_loss: 0.3384 - val_accuracy: 0.8774\n",
      "Epoch 10/100\n",
      "858/858 [==============================] - 23s 27ms/step - loss: 0.2336 - accuracy: 0.9085 - val_loss: 0.3535 - val_accuracy: 0.8751\n",
      "Epoch 11/100\n",
      "858/858 [==============================] - 23s 27ms/step - loss: 0.2179 - accuracy: 0.9142 - val_loss: 0.3535 - val_accuracy: 0.8738\n",
      "Epoch 12/100\n",
      "858/858 [==============================] - 23s 27ms/step - loss: 0.2074 - accuracy: 0.9166 - val_loss: 0.3466 - val_accuracy: 0.8846\n",
      "Epoch 13/100\n",
      "858/858 [==============================] - 23s 27ms/step - loss: 0.1905 - accuracy: 0.9244 - val_loss: 0.3545 - val_accuracy: 0.8846\n",
      "Epoch 14/100\n",
      "858/858 [==============================] - 23s 27ms/step - loss: 0.1840 - accuracy: 0.9259 - val_loss: 0.3550 - val_accuracy: 0.8850\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.8291 - accuracy: 0.8012\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2b9e78d30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2b9e78d30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: nn_partially_propagated_2000-clusters_50-percentile/assets\n",
      "Percentile=  75\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x15f9603a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x15f9603a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1258/1260 [============================>.] - ETA: 0s - loss: 0.7223 - accuracy: 0.7382WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2c1a5e280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x2c1a5e280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1260/1260 [==============================] - 34s 27ms/step - loss: 0.7219 - accuracy: 0.7384 - val_loss: 0.4230 - val_accuracy: 0.8406\n",
      "Epoch 2/100\n",
      "1260/1260 [==============================] - 34s 27ms/step - loss: 0.4242 - accuracy: 0.8402 - val_loss: 0.3989 - val_accuracy: 0.8426\n",
      "Epoch 3/100\n",
      "1260/1260 [==============================] - 33s 27ms/step - loss: 0.3839 - accuracy: 0.8546 - val_loss: 0.3880 - val_accuracy: 0.8497\n",
      "Epoch 4/100\n",
      "1260/1260 [==============================] - 34s 27ms/step - loss: 0.3530 - accuracy: 0.8647 - val_loss: 0.3594 - val_accuracy: 0.8640\n",
      "Epoch 5/100\n",
      "1260/1260 [==============================] - 34s 27ms/step - loss: 0.3325 - accuracy: 0.8727 - val_loss: 0.3502 - val_accuracy: 0.8689\n",
      "Epoch 6/100\n",
      "1260/1260 [==============================] - 34s 27ms/step - loss: 0.3094 - accuracy: 0.8785 - val_loss: 0.3515 - val_accuracy: 0.8662\n",
      "Epoch 7/100\n",
      "1260/1260 [==============================] - 34s 27ms/step - loss: 0.3015 - accuracy: 0.8793 - val_loss: 0.3440 - val_accuracy: 0.8687\n",
      "Epoch 8/100\n",
      "1260/1260 [==============================] - 33s 26ms/step - loss: 0.2843 - accuracy: 0.8879 - val_loss: 0.3585 - val_accuracy: 0.8656\n",
      "Epoch 9/100\n",
      "1260/1260 [==============================] - 34s 27ms/step - loss: 0.2719 - accuracy: 0.8899 - val_loss: 0.3358 - val_accuracy: 0.8698\n",
      "Epoch 10/100\n",
      "1260/1260 [==============================] - 34s 27ms/step - loss: 0.2565 - accuracy: 0.8986 - val_loss: 0.3580 - val_accuracy: 0.8636\n",
      "Epoch 11/100\n",
      "1260/1260 [==============================] - 34s 27ms/step - loss: 0.2460 - accuracy: 0.9005 - val_loss: 0.3294 - val_accuracy: 0.8776\n",
      "Epoch 12/100\n",
      "1260/1260 [==============================] - 34s 27ms/step - loss: 0.2260 - accuracy: 0.9104 - val_loss: 0.3476 - val_accuracy: 0.8696\n",
      "Epoch 13/100\n",
      "1260/1260 [==============================] - 34s 27ms/step - loss: 0.2131 - accuracy: 0.9136 - val_loss: 0.3595 - val_accuracy: 0.8718\n",
      "Epoch 14/100\n",
      "1260/1260 [==============================] - 33s 27ms/step - loss: 0.2049 - accuracy: 0.9179 - val_loss: 0.3893 - val_accuracy: 0.8703\n",
      "Epoch 15/100\n",
      "1260/1260 [==============================] - 34s 27ms/step - loss: 0.1909 - accuracy: 0.9230 - val_loss: 0.3966 - val_accuracy: 0.8667\n",
      "Epoch 16/100\n",
      "1260/1260 [==============================] - 34s 27ms/step - loss: 0.1823 - accuracy: 0.9271 - val_loss: 0.4046 - val_accuracy: 0.8736\n",
      "Epoch 17/100\n",
      "1260/1260 [==============================] - 34s 27ms/step - loss: 0.1712 - accuracy: 0.9311 - val_loss: 0.4103 - val_accuracy: 0.8714\n",
      "Epoch 18/100\n",
      "1260/1260 [==============================] - 34s 27ms/step - loss: 0.1599 - accuracy: 0.9356 - val_loss: 0.4360 - val_accuracy: 0.8658\n",
      "Epoch 19/100\n",
      "1260/1260 [==============================] - 34s 27ms/step - loss: 0.1546 - accuracy: 0.9373 - val_loss: 0.4577 - val_accuracy: 0.8678\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 1.0009 - accuracy: 0.7960\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2bb37e280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2bb37e280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: nn_partially_propagated_2000-clusters_75-percentile/assets\n",
      "peak memory: 3669.38 MiB, increment: 2027.27 MiB\n",
      "CPU times: user 45min 17s, sys: 1min 11s, total: 46min 28s\n",
      "Wall time: 27min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "for i in [20,25,30,50,75]:\n",
    "    print('Percentile= ',i)\n",
    "    cluster_label_prop(k=2000,percentile_closest=i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
